{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decimal-belle",
   "metadata": {},
   "source": [
    "### Objective: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "conservative-lodge",
   "metadata": {},
   "source": [
    "Objective of this project is to use Spark NLP for multi-class text classification on the twitter sentiment dataset provided in the CSV data file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-twist",
   "metadata": {},
   "source": [
    "#### Starting pyspark with spark nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "collectible-prime",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import packages\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "sc =SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-repository",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "signal-folks",
   "metadata": {},
   "source": [
    "Attachment_1635667433.csv file has been used as the input for this assignment.\n",
    "The dataset contains below columns:\n",
    "1) Location - Location of the user\n",
    "2) TweetAt - Time of the tweet\n",
    "3) OriginalTweet - Tweet by the user\n",
    "4) Label - Sentiment Class of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-antarctica",
   "metadata": {},
   "source": [
    "Copying the data to Hadoop"
   ]
  },
  {
   "cell_type": "raw",
   "id": "absent-occupation",
   "metadata": {},
   "source": [
    "hadoop fs -put Attachment_1635667433.csv /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-belarus",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "inappropriate-exploration",
   "metadata": {},
   "source": [
    "The input CSV file has been loaded with spark CSV read format as shown below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conditional-convert",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68046"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File location and type\n",
    "file_location = r'Attachment_1635667433.csv'\n",
    "file_type = \"csv\"# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "df = sqlContext.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-fellowship",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "experimental-appointment",
   "metadata": {},
   "source": [
    "Below are the basic datafile statistics where we check the data types, sample data, about the values in the data columns and drop missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "opponent-teens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- UserName: string (nullable = true)\n",
      " |-- ScreenName: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- TweetAt: string (nullable = true)\n",
      " |-- OriginalTweet: string (nullable = true)\n",
      " |-- Sentiment: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "under-professor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------------------+----------+--------------------+---------+\n",
      "|UserName|  ScreenName|            Location|   TweetAt|       OriginalTweet|Sentiment|\n",
      "+--------+------------+--------------------+----------+--------------------+---------+\n",
      "|    3799|       48751|              London|16-03-2020|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|    3800|       48752|                  UK|16-03-2020|advice Talk to yo...| Positive|\n",
      "|    3801|       48753|           Vagabonds|16-03-2020|Coronavirus Austr...| Positive|\n",
      "|    3802|       48754|                null|16-03-2020|My food stock is ...|     null|\n",
      "|  PLEASE| don't panic| THERE WILL BE EN...|      null|                null|     null|\n",
      "+--------+------------+--------------------+----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chief-chair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|       OriginalTweet|Sentiment|\n",
      "+--------------------+---------+\n",
      "|@MeNyrbie @Phil_G...|  Neutral|\n",
      "|advice Talk to yo...| Positive|\n",
      "|Coronavirus Austr...| Positive|\n",
      "|My food stock is ...|     null|\n",
      "|                null|     null|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Removing unwanted columns\n",
    "unwanted_cols = ['UserName', 'ScreenName', 'Location', 'TweetAt']\n",
    "df1 = df.select([column for column in df.columns if column not in unwanted_cols])\n",
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "maritime-triumph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28617"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping nulls from the data\n",
    "df1 = df1.dropna()\n",
    "df1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "serious-blues",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|           Sentiment|count|\n",
      "+--------------------+-----+\n",
      "|            Positive| 7718|\n",
      "|            Negative| 6857|\n",
      "|             Neutral| 5224|\n",
      "|  Extremely Positive| 4412|\n",
      "|  Extremely Negative| 3751|\n",
      "|   social distancing|    5|\n",
      "|    N. Y. - April 10|    3|\n",
      "| state governors ...|    2|\n",
      "|             however|    2|\n",
      "| supermarket workers|    2|\n",
      "|        Stay with us|    2|\n",
      "| but we also need...|    2|\n",
      "| or click the lin...|    2|\n",
      "| just \"\"selfish p...|    2|\n",
      "|           of course|    2|\n",
      "| not going to the...|    2|\n",
      "| ecological collapse|    2|\n",
      "|        Corona Virus|    2|\n",
      "|            delivery|    2|\n",
      "| Big Tech could b...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count of sentiment classes\n",
    "from pyspark.sql.functions import col\n",
    "df1.groupBy(\"Sentiment\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "printable-wedding",
   "metadata": {},
   "source": [
    "From the above output, we can see that the Sentiment column contains lots of unwanted or misplaced data which we have cleaned as to get required data of interest only for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "optional-track",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|         Sentiment|count|\n",
      "+------------------+-----+\n",
      "|          Positive| 7718|\n",
      "|          Negative| 6857|\n",
      "|           Neutral| 5224|\n",
      "|Extremely Positive| 4412|\n",
      "|Extremely Negative| 3751|\n",
      "+------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtering sentiment classes\n",
    "import pyspark.sql.functions as f\n",
    "df2 = df1.where(f.col(\"Sentiment\").isin([\"Positive\", \"Negative\", \"Neutral\",\"Extremely Positive\",\"Extremely Negative\"]))\n",
    "df2.groupBy(\"Sentiment\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-arthritis",
   "metadata": {},
   "source": [
    "### Data splitting into training and testing data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "realistic-purple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70% for training, 30% for testing\n",
    "(trainingData, testData) = df2.randomSplit([0.7, 0.3], seed = 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-defensive",
   "metadata": {},
   "source": [
    "### Spark NLP model pipleine creation "
   ]
  },
  {
   "cell_type": "raw",
   "id": "actual-union",
   "metadata": {},
   "source": [
    "At this stage, we create the spark NLP model pipeline which involves transforming the raw text data to model understandable format using NLP techniques such as regexTokenizer, removing stop words, labelling string indexes as shown below. After the data has been transformed using NLP techniques we add one more stage in the pipleine which will be the classification modelling stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "premium-dollar",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, OneHotEncoder, StringIndexer, VectorAssembler, IndexToString\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator# convert text column to spark nlp document\n",
    "\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"OriginalTweet\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "label_stringIdx = StringIndexer(inputCol = \"Sentiment\", outputCol = \"label\")\n",
    "# Model definiton\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.0)# To convert index(integer) to corresponding class labels\n",
    "label_to_stringIdx = IndexToString(inputCol=\"label\", outputCol=\"sentiment_class\")\n",
    "\n",
    "# NLP pipeline definiton\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[regexTokenizer, \n",
    "            stopwordsRemover, \n",
    "            countVectors, \n",
    "            label_stringIdx,\n",
    "            lr,\n",
    "            label_to_stringIdx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "experienced-tiger",
   "metadata": {},
   "source": [
    "### Model Creation using spark NLP pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "forced-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the pipeline on training data\n",
    "pipeline_model = nlp_pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-latest",
   "metadata": {},
   "source": [
    "### Generate predictions on the test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bizarre-lightning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform predictions on test data\n",
    "predictions =  pipeline_model.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worthy-miniature",
   "metadata": {},
   "source": [
    "### Model Performance measurement "
   ]
  },
  {
   "cell_type": "raw",
   "id": "random-collection",
   "metadata": {},
   "source": [
    "As this is the multi-class classification model, the model performance has been measured using Accuracy, Error, Precison and Recall on the test dataset predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "crude-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.493003\n",
      "Test Error = 0.506997 \n"
     ]
    }
   ],
   "source": [
    "# import evaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "maritime-dinner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.524037\n",
      "Test Error = 0.475963 \n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "informative-assurance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.493003\n",
      "Test Error = 0.506997 \n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy = %g\" % (accuracy))\n",
    "print(\"Test Error = %g \" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-carry",
   "metadata": {},
   "source": [
    "### Conclusion: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "gentle-leonard",
   "metadata": {},
   "source": [
    "In this project, which have understood the raw input data was having lots of null values and misplaced values for the \"Sentiment\" column with which we can conclude that the quality of the data was poor for modelling. We have applied multi-class classification model using logistic regression on the transformed data and the model is having ~50% performance accuracy in predicting the test data which is average due to the data quality.\n",
    "\n",
    "This project helps to understand end to end stages in building text classification models using pyspark and spark nlp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-circulation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
